{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import time\n",
    "from models import create_model\n",
    "from util.writer import Writer\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "from util import util\n",
    "import torch\n",
    "import sys\n",
    "CUDA_LAUNCH_BLOCKING=\"1\"\n",
    "from os.path import join\n",
    "from models import networks\n",
    "from torch import nn\n",
    "from util.util import seg_accuracy, print_network\n",
    "from models.layers.mesh_conv import MeshConv\n",
    "import functools\n",
    "from models.layers.mesh_pool import MeshPool\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "import torch.utils.data as data\n",
    "from util.util import is_mesh_file, pad\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Parser:  To load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOptions:\n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self):\n",
    "        # data params\n",
    "        self.parser.add_argument('--dataroot', default = 'datasets/shrec_16', help='path to meshes (should have subfolders train, test)')\n",
    "        self.parser.add_argument('--dataset_mode', choices={\"classification\", \"segmentation\"}, default='classification')\n",
    "        self.parser.add_argument('--ninput_edges', type=int, default=750, help='# of input edges (will include dummy edges)')\n",
    "        self.parser.add_argument('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples per epoch')\n",
    "        # network params\n",
    "        self.parser.add_argument('--batch_size', type=int, default=16, help='input batch size')\n",
    "        self.parser.add_argument('--arch', type=str, default='mconvnet', help='selects network to use') #todo add choices\n",
    "        self.parser.add_argument('--resblocks', type=int, default=1, help='# of res blocks')\n",
    "        self.parser.add_argument('--fc_n', type=int, default=100, help='# between fc and nclasses') #todo make generic\n",
    "        self.parser.add_argument('--ncf', nargs='+', default=[64, 128, 256, 256], type=int, help='conv filters')\n",
    "        self.parser.add_argument('--pool_res', nargs='+', default= [600, 450, 300, 180], type=int, help='pooling res')\n",
    "        self.parser.add_argument('--norm', type=str, default='group',help='instance normalization or batch normalization or group normalization')\n",
    "        self.parser.add_argument('--num_groups', type=int, default=16, help='# of groups for groupnorm')\n",
    "        self.parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
    "        self.parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
    "        # general params\n",
    "        self.parser.add_argument('--num_threads', default=3, type=int, help='# threads for loading data')\n",
    "        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "        self.parser.add_argument('--name', type=str, default='debug', help='name of the experiment. It decides where to store samples and models')\n",
    "        self.parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
    "        self.parser.add_argument('--serial_batches', action='store_true', help='if true, takes meshes in order, otherwise takes them randomly')\n",
    "        self.parser.add_argument('--seed', type=int, help='if specified, uses seed')\n",
    "        # visualization params\n",
    "        self.parser.add_argument('--export_folder', type=str, default='', help='exports intermediate collapses to this folder')\n",
    "        #\n",
    "        self.initialized = True\n",
    "\n",
    "    def parse(self):\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        self.opt, unknown = self.parser.parse_known_args()\n",
    "        self.opt.is_train = self.is_train   # train or test\n",
    "\n",
    "        str_ids = self.opt.gpu_ids.split(',')\n",
    "        self.opt.gpu_ids = []\n",
    "        for str_id in str_ids:\n",
    "            id = int(str_id)\n",
    "            if id >= 0:\n",
    "                self.opt.gpu_ids.append(id)\n",
    "        # set gpu ids\n",
    "        if len(self.opt.gpu_ids) > 0:\n",
    "            torch.cuda.set_device(self.opt.gpu_ids[0])\n",
    "\n",
    "        args = vars(self.opt)\n",
    "\n",
    "        if self.opt.seed is not None:\n",
    "            import numpy as np\n",
    "            import random\n",
    "            torch.manual_seed(self.opt.seed)\n",
    "            np.random.seed(self.opt.seed)\n",
    "            random.seed(self.opt.seed)\n",
    "\n",
    "        if self.opt.export_folder:\n",
    "            self.opt.export_folder = os.path.join(self.opt.checkpoints_dir, self.opt.name, self.opt.export_folder)\n",
    "            util.mkdir(self.opt.export_folder)\n",
    "\n",
    "        if self.is_train:\n",
    "            print('------------ Options -------------')\n",
    "            for k, v in sorted(args.items()):\n",
    "                print('%s: %s' % (str(k), str(v)))\n",
    "            print('-------------- End ----------------')\n",
    "\n",
    "            # save to the disk\n",
    "            expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n",
    "            util.mkdir(expr_dir)\n",
    "\n",
    "            file_name = os.path.join(expr_dir, 'opt.txt')\n",
    "            with open(file_name, 'wt') as opt_file:\n",
    "                opt_file.write('------------ Options -------------\\n')\n",
    "                for k, v in sorted(args.items()):\n",
    "                    opt_file.write('%s: %s\\n' % (str(k), str(v)))\n",
    "                opt_file.write('-------------- End ----------------\\n')\n",
    "        return self.opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Train Options of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainOptions(BaseOptions):\n",
    "    def initialize(self):\n",
    "        BaseOptions.initialize(self)\n",
    "        self.parser.add_argument('--print_freq', type=int, default=10, help='frequency of showing training results on console')\n",
    "        self.parser.add_argument('--save_latest_freq', type=int, default=250, help='frequency of saving the latest results')\n",
    "        self.parser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\n",
    "        self.parser.add_argument('--run_test_freq', type=int, default=1, help='frequency of running test in training script')\n",
    "        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
    "        self.parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
    "        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
    "        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
    "        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n",
    "        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n",
    "        self.parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')\n",
    "        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
    "        self.parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n",
    "        self.parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n",
    "        # data augmentation stuff\n",
    "        self.parser.add_argument('--num_aug', type=int, default=20, help='# of augmentation files')\n",
    "        self.parser.add_argument('--scale_verts', action='store_true', help='non-uniformly scale the mesh e.g., in x, y or z')\n",
    "        self.parser.add_argument('--slide_verts', type=float, default=0.2, help='percent vertices which will be shifted along the mesh surface')\n",
    "        self.parser.add_argument('--flip_edges', type=float, default=0.2, help='percent of edges to randomly flip')\n",
    "        # tensorboard visualization\n",
    "        self.parser.add_argument('--no_vis', action='store_true', help='will not use tensorboard')\n",
    "        self.parser.add_argument('--verbose_plot', action='store_true', help='plots network weights, etc.')\n",
    "        self.is_train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.mean = 0\n",
    "        self.std = 1\n",
    "        self.ninput_channels = None\n",
    "        super(BaseDataset, self).__init__()\n",
    "\n",
    "    def get_mean_std(self):\n",
    "        \"\"\" Computes Mean and Standard Deviation from Training Data\n",
    "        If mean/std file doesn't exist, will compute one\n",
    "        :returns\n",
    "        mean: N-dimensional mean\n",
    "        std: N-dimensional standard deviation\n",
    "        ninput_channels: N\n",
    "        (here N=5)\n",
    "        \"\"\"\n",
    "\n",
    "        mean_std_cache = os.path.join(self.root, 'mean_std_cache.p')\n",
    "        if not os.path.isfile(mean_std_cache):\n",
    "            print('computing mean std from train data...')\n",
    "            # doesn't run augmentation during m/std computation\n",
    "            num_aug = self.opt.num_aug\n",
    "            self.opt.num_aug = 1\n",
    "            mean, std = np.array(0), np.array(0)\n",
    "            for i, data in enumerate(self):\n",
    "                if i % 500 == 0:\n",
    "                    print('{} of {}'.format(i, self.size))\n",
    "                features = data['edge_features']\n",
    "                mean = mean + features.mean(axis=1)\n",
    "                std = std + features.std(axis=1)\n",
    "            mean = mean / (i + 1)\n",
    "            std = std / (i + 1)\n",
    "            transform_dict = {'mean': mean[:, np.newaxis], 'std': std[:, np.newaxis],\n",
    "                              'ninput_channels': len(mean)}\n",
    "            with open(mean_std_cache, 'wb') as f:\n",
    "                pickle.dump(transform_dict, f)\n",
    "            print('saved: ', mean_std_cache)\n",
    "            self.opt.num_aug = num_aug\n",
    "        # open mean / std from file\n",
    "        with open(mean_std_cache, 'rb') as f:\n",
    "            transform_dict = pickle.load(f)\n",
    "            print('loaded mean / std from cache')\n",
    "            self.mean = transform_dict['mean']\n",
    "            self.std = transform_dict['std']\n",
    "            self.ninput_channels = transform_dict['ninput_channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(opt):\n",
    "    \"\"\"loads dataset class\"\"\"\n",
    "\n",
    "    if opt.dataset_mode == 'segmentation':\n",
    "        from data.segmentation_data import SegmentationData\n",
    "        dataset = SegmentationData(opt)\n",
    "    elif opt.dataset_mode == 'classification':\n",
    "        from data.classification_data import ClassificationData\n",
    "        dataset = ClassificationData(opt)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main function for obtaining ClassificationData model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader Class: \n",
    "\n",
    "(A) An Interesting point is that we call the yield function instead of return function. Yield helps return in batches instead of returning by idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"multi-threaded data loading\"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.dataset = CreateDataset(opt)\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=not opt.serial_batches,\n",
    "            num_workers=int(opt.num_threads),\n",
    "            collate_fn=collate_fn)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset), self.opt.max_dataset_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, data in enumerate(self.dataloader):\n",
    "            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n",
    "                break\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Creates mini-batch tensors\n",
    "    We should build custom collate_fn rather than using default collate_fn\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "    keys = batch[0].keys()\n",
    "    for key in keys:\n",
    "        meta.update({key: np.array([d[key] for d in batch])})\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "arch: mconvnet\n",
      "batch_size: 16\n",
      "beta1: 0.9\n",
      "checkpoints_dir: ./checkpoints\n",
      "continue_train: False\n",
      "dataroot: datasets/shrec_16\n",
      "dataset_mode: classification\n",
      "epoch_count: 1\n",
      "export_folder: \n",
      "fc_n: 100\n",
      "flip_edges: 0.2\n",
      "gpu_ids: [0]\n",
      "init_gain: 0.02\n",
      "init_type: normal\n",
      "is_train: True\n",
      "lr: 0.0002\n",
      "lr_decay_iters: 50\n",
      "lr_policy: lambda\n",
      "max_dataset_size: inf\n",
      "name: debug\n",
      "ncf: [64, 128, 256, 256]\n",
      "ninput_edges: 750\n",
      "niter: 100\n",
      "niter_decay: 100\n",
      "no_vis: False\n",
      "norm: group\n",
      "num_aug: 20\n",
      "num_groups: 16\n",
      "num_threads: 3\n",
      "phase: train\n",
      "pool_res: [600, 450, 300, 180]\n",
      "print_freq: 10\n",
      "resblocks: 1\n",
      "run_test_freq: 1\n",
      "save_epoch_freq: 1\n",
      "save_latest_freq: 250\n",
      "scale_verts: False\n",
      "seed: None\n",
      "serial_batches: False\n",
      "slide_verts: 0.2\n",
      "verbose_plot: False\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "opt = TrainOptions().parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fill mesh Function: This function fills the mesh in absence of regular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mesh(mesh2fill, file: str, opt):\n",
    "    \n",
    "    load_path = get_mesh_path(file, opt.num_aug)\n",
    "    print('Load Path is', load_path)\n",
    "    if os.path.exists(load_path):\n",
    "        print('Path Exists: Loading the mesh data')\n",
    "        mesh_data = np.load(load_path, encoding='latin1', allow_pickle=True)\n",
    "    else:\n",
    "        mesh_data = from_scratch(file, opt)\n",
    "        np.savez_compressed(load_path, gemm_edges=mesh_data.gemm_edges, vs=mesh_data.vs, edges=mesh_data.edges,\n",
    "                            edges_count=mesh_data.edges_count, ve=mesh_data.ve, v_mask=mesh_data.v_mask,\n",
    "                            filename=mesh_data.filename, sides=mesh_data.sides,\n",
    "                            edge_lengths=mesh_data.edge_lengths, edge_areas=mesh_data.edge_areas,\n",
    "                            features=mesh_data.features)\n",
    "    mesh2fill.vs = mesh_data['vs']\n",
    "    mesh2fill.edges = mesh_data['edges']\n",
    "    mesh2fill.gemm_edges = mesh_data['gemm_edges']\n",
    "    mesh2fill.edges_count = int(mesh_data['edges_count'])\n",
    "    mesh2fill.ve = mesh_data['ve']\n",
    "    mesh2fill.v_mask = mesh_data['v_mask']\n",
    "    mesh2fill.filename = str(mesh_data['filename'])\n",
    "    mesh2fill.edge_lengths = mesh_data['edge_lengths']\n",
    "    mesh2fill.edge_areas = mesh_data['edge_areas']\n",
    "    mesh2fill.features = mesh_data['features']\n",
    "    mesh2fill.sides = mesh_data['sides']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Mesh Path: This tells the directory of the path to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh_path(file: str, num_aug: int):\n",
    "    filename, _ = os.path.splitext(file)\n",
    "    dir_name = os.path.dirname(filename)\n",
    "    prefix = os.path.basename(filename)\n",
    "    load_dir = os.path.join(dir_name, 'cache')\n",
    "    load_file = os.path.join(load_dir, '%s_%03d.npz' % (prefix, np.random.randint(0, num_aug)))\n",
    "    if not os.path.isdir(load_dir):\n",
    "        os.makedirs(load_dir, exist_ok=True)\n",
    "    return load_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationData(BaseDataset):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        BaseDataset.__init__(self, opt)\n",
    "        self.opt = opt\n",
    "        self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')\n",
    "        self.root = opt.dataroot\n",
    "        self.dir = os.path.join(opt.dataroot)\n",
    "        self.classes, self.class_to_idx = self.find_classes(self.dir)\n",
    "        self.paths = self.make_dataset_by_class(self.dir, self.class_to_idx, opt.phase)\n",
    "        self.nclasses = len(self.classes)\n",
    "        self.size = len(self.paths)\n",
    "        self.get_mean_std()\n",
    "        # modify for network later.\n",
    "        opt.nclasses = self.nclasses\n",
    "        opt.input_nc = self.ninput_channels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index][0]\n",
    "        label = self.paths[index][1]\n",
    "        mesh = Mesh(file=path, opt=self.opt, hold_history=False, export_folder=self.opt.export_folder)\n",
    "        meta = {'mesh': mesh, 'label': label}\n",
    "        # get edge features\n",
    "        edge_features = mesh.extract_features()\n",
    "        edge_features = pad(edge_features, self.opt.ninput_edges)\n",
    "        meta['edge_features'] = (edge_features - self.mean) / self.std\n",
    "        return meta\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # this is when the folders are organized by class...\n",
    "    @staticmethod\n",
    "    def find_classes(dir):\n",
    "        classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dataset_by_class(dir, class_to_idx, phase):\n",
    "        meshes = []\n",
    "        dir = os.path.expanduser(dir)\n",
    "        for target in sorted(os.listdir(dir)):\n",
    "            d = os.path.join(dir, target)\n",
    "            if not os.path.isdir(d):\n",
    "                continue\n",
    "            for root, _, fnames in sorted(os.walk(d)):\n",
    "                for fname in sorted(fnames):\n",
    "                    if is_mesh_file(fname) and (root.count(phase)==1):\n",
    "                        path = os.path.join(root, fname)\n",
    "                        item = (path, class_to_idx[target])\n",
    "                        meshes.append(item)\n",
    "        return meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesh Class: The fundamental building block for Mesh CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mesh:\n",
    "\n",
    "    def __init__(self, file=None, opt=None, hold_history=False, export_folder=''):\n",
    "        self.vs = self.v_mask = self.filename = self.features = self.edge_areas = None\n",
    "        self.edges = self.gemm_edges = self.sides = None\n",
    "        self.pool_count = 0\n",
    "        print('Filling the Values onto the Mesh')\n",
    "        fill_mesh(self, file, opt)\n",
    "        self.export_folder = export_folder\n",
    "        self.history_data = None\n",
    "        print('Hold History is', hold_history)\n",
    "        if hold_history:\n",
    "            self.init_history()\n",
    "        self.export()\n",
    "\n",
    "    def extract_features(self):\n",
    "        print('Returning the features')\n",
    "        return self.features\n",
    "\n",
    "    def merge_vertices(self, edge_id):\n",
    "        self.remove_edge(edge_id)\n",
    "        edge = self.edges[edge_id]\n",
    "        v_a = self.vs[edge[0]]\n",
    "        v_b = self.vs[edge[1]]\n",
    "        # update pA\n",
    "        v_a.__iadd__(v_b)\n",
    "        v_a.__itruediv__(2)\n",
    "        self.v_mask[edge[1]] = False\n",
    "        mask = self.edges == edge[1]\n",
    "        self.ve[edge[0]].extend(self.ve[edge[1]])\n",
    "        self.edges[mask] = edge[0]\n",
    "\n",
    "    def remove_vertex(self, v):\n",
    "        self.v_mask[v] = False\n",
    "\n",
    "    def remove_edge(self, edge_id):\n",
    "        vs = self.edges[edge_id]\n",
    "        for v in vs:\n",
    "            if edge_id not in self.ve[v]:\n",
    "                print(self.ve[v])\n",
    "                print(self.filename)\n",
    "            self.ve[v].remove(edge_id)\n",
    "\n",
    "    def clean(self, edges_mask, groups):\n",
    "        edges_mask = edges_mask.astype(bool)\n",
    "        torch_mask = torch.from_numpy(edges_mask.copy())\n",
    "        self.gemm_edges = self.gemm_edges[edges_mask]\n",
    "        self.edges = self.edges[edges_mask]\n",
    "        self.sides = self.sides[edges_mask]\n",
    "        new_ve = []\n",
    "        edges_mask = np.concatenate([edges_mask, [False]])\n",
    "        new_indices = np.zeros(edges_mask.shape[0], dtype=np.int32)\n",
    "        new_indices[-1] = -1\n",
    "        new_indices[edges_mask] = np.arange(0, np.ma.where(edges_mask)[0].shape[0])\n",
    "        self.gemm_edges[:, :] = new_indices[self.gemm_edges[:, :]]\n",
    "        for v_index, ve in enumerate(self.ve):\n",
    "            update_ve = []\n",
    "            # if self.v_mask[v_index]:\n",
    "            for e in ve:\n",
    "                update_ve.append(new_indices[e])\n",
    "            new_ve.append(update_ve)\n",
    "        self.ve = new_ve\n",
    "        self.__clean_history(groups, torch_mask)\n",
    "        self.pool_count += 1\n",
    "        self.export()\n",
    "\n",
    "\n",
    "    def export(self, file=None, vcolor=None):\n",
    "        print('Calling the Export Function')\n",
    "        if file is None:\n",
    "            if self.export_folder:\n",
    "                filename, file_extension = os.path.splitext(self.filename)\n",
    "                file = '%s/%s_%d%s' % (self.export_folder, filename, self.pool_count, file_extension)\n",
    "            else:\n",
    "                return\n",
    "        faces = []\n",
    "        vs = self.vs[self.v_mask]\n",
    "        gemm = np.array(self.gemm_edges)\n",
    "        new_indices = np.zeros(self.v_mask.shape[0], dtype=np.int32)\n",
    "        new_indices[self.v_mask] = np.arange(0, np.ma.where(self.v_mask)[0].shape[0])\n",
    "        for edge_index in range(len(gemm)):\n",
    "            print('Checking for Cycles in the graph')\n",
    "            cycles = self.__get_cycle(gemm, edge_index)\n",
    "            for cycle in cycles:\n",
    "                faces.append(self.__cycle_to_face(cycle, new_indices))\n",
    "        with open(file, 'w+') as f:\n",
    "            for vi, v in enumerate(vs):\n",
    "                vcol = ' %f %f %f' % (vcolor[vi, 0], vcolor[vi, 1], vcolor[vi, 2]) if vcolor is not None else ''\n",
    "                f.write(\"v %f %f %f%s\\n\" % (v[0], v[1], v[2], vcol))\n",
    "            for face_id in range(len(faces) - 1):\n",
    "                f.write(\"f %d %d %d\\n\" % (faces[face_id][0] + 1, faces[face_id][1] + 1, faces[face_id][2] + 1))\n",
    "            f.write(\"f %d %d %d\" % (faces[-1][0] + 1, faces[-1][1] + 1, faces[-1][2] + 1))\n",
    "            for edge in self.edges:\n",
    "                f.write(\"\\ne %d %d\" % (new_indices[edge[0]] + 1, new_indices[edge[1]] + 1))\n",
    "\n",
    "    def export_segments(self, segments):\n",
    "        if not self.export_folder:\n",
    "            return\n",
    "        cur_segments = segments\n",
    "        for i in range(self.pool_count + 1):\n",
    "            filename, file_extension = os.path.splitext(self.filename)\n",
    "            file = '%s/%s_%d%s' % (self.export_folder, filename, i, file_extension)\n",
    "            fh, abs_path = mkstemp()\n",
    "            edge_key = 0\n",
    "            with os.fdopen(fh, 'w') as new_file:\n",
    "                with open(file) as old_file:\n",
    "                    for line in old_file:\n",
    "                        if line[0] == 'e':\n",
    "                            new_file.write('%s %d' % (line.strip(), cur_segments[edge_key]))\n",
    "                            if edge_key < len(cur_segments):\n",
    "                                edge_key += 1\n",
    "                                new_file.write('\\n')\n",
    "                        else:\n",
    "                            new_file.write(line)\n",
    "            os.remove(file)\n",
    "            move(abs_path, file)\n",
    "            if i < len(self.history_data['edges_mask']):\n",
    "                cur_segments = segments[:len(self.history_data['edges_mask'][i])]\n",
    "                cur_segments = cur_segments[self.history_data['edges_mask'][i]]\n",
    "\n",
    "    def __get_cycle(self, gemm, edge_id):\n",
    "        cycles = []\n",
    "        for j in range(2):\n",
    "            next_side = start_point = j * 2\n",
    "            next_key = edge_id\n",
    "            if gemm[edge_id, start_point] == -1:\n",
    "                continue\n",
    "            cycles.append([])\n",
    "            for i in range(3):\n",
    "                tmp_next_key = gemm[next_key, next_side]\n",
    "                tmp_next_side = self.sides[next_key, next_side]\n",
    "                tmp_next_side = tmp_next_side + 1 - 2 * (tmp_next_side % 2)\n",
    "                gemm[next_key, next_side] = -1\n",
    "                gemm[next_key, next_side + 1 - 2 * (next_side % 2)] = -1\n",
    "                next_key = tmp_next_key\n",
    "                next_side = tmp_next_side\n",
    "                cycles[-1].append(next_key)\n",
    "        return cycles\n",
    "\n",
    "    def __cycle_to_face(self, cycle, v_indices):\n",
    "        face = []\n",
    "        for i in range(3):\n",
    "            v = list(set(self.edges[cycle[i]]) & set(self.edges[cycle[(i + 1) % 3]]))[0]\n",
    "            face.append(v_indices[v])\n",
    "        return face\n",
    "\n",
    "    def init_history(self):\n",
    "        self.history_data = {\n",
    "                               'groups': [],\n",
    "                               'gemm_edges': [self.gemm_edges.copy()],\n",
    "                               'occurrences': [],\n",
    "                               'old2current': np.arange(self.edges_count, dtype=np.int32),\n",
    "                               'current2old': np.arange(self.edges_count, dtype=np.int32),\n",
    "                               'edges_mask': [torch.ones(self.edges_count,dtype=torch.bool)],\n",
    "                               'edges_count': [self.edges_count],\n",
    "                              }\n",
    "        if self.export_folder:\n",
    "            self.history_data['collapses'] = MeshUnion(self.edges_count)\n",
    "\n",
    "    def union_groups(self, source, target):\n",
    "        if self.export_folder and self.history_data:\n",
    "            self.history_data['collapses'].union(self.history_data['current2old'][source], self.history_data['current2old'][target])\n",
    "        return\n",
    "\n",
    "    def remove_group(self, index):\n",
    "        if self.history_data is not None:\n",
    "            self.history_data['edges_mask'][-1][self.history_data['current2old'][index]] = 0\n",
    "            self.history_data['old2current'][self.history_data['current2old'][index]] = -1\n",
    "            if self.export_folder:\n",
    "                self.history_data['collapses'].remove_group(self.history_data['current2old'][index])\n",
    "\n",
    "    def get_groups(self):\n",
    "        return self.history_data['groups'].pop()\n",
    "\n",
    "    def get_occurrences(self):\n",
    "        return self.history_data['occurrences'].pop()\n",
    "    \n",
    "    def __clean_history(self, groups, pool_mask):\n",
    "        if self.history_data is not None:\n",
    "            mask = self.history_data['old2current'] != -1\n",
    "            self.history_data['old2current'][mask] = np.arange(self.edges_count, dtype=np.int32)\n",
    "            self.history_data['current2old'][0: self.edges_count] = np.ma.where(mask)[0]\n",
    "            if self.export_folder != '':\n",
    "                self.history_data['edges_mask'].append(self.history_data['edges_mask'][-1].clone())\n",
    "            self.history_data['occurrences'].append(groups.get_occurrences())\n",
    "            self.history_data['groups'].append(groups.get_groups(pool_mask))\n",
    "            self.history_data['gemm_edges'].append(self.gemm_edges.copy())\n",
    "            self.history_data['edges_count'].append(self.edges_count)\n",
    "    \n",
    "    def unroll_gemm(self):\n",
    "        self.history_data['gemm_edges'].pop()\n",
    "        self.gemm_edges = self.history_data['gemm_edges'][-1]\n",
    "        self.history_data['edges_count'].pop()\n",
    "        self.edges_count = self.history_data['edges_count'][-1]\n",
    "\n",
    "    def get_edge_areas(self):\n",
    "        return self.edge_areas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Individual Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) CreatingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n"
     ]
    }
   ],
   "source": [
    "dataset = ClassificationData(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Obtaining Parameters from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "The Device being using is cuda:0\n",
      "The Root Path is datasets/shrec_16\n",
      "Directory is datasets/shrec_16\n",
      "\n",
      "Classes are: ['alien', 'ants', 'armadillo', 'bird1', 'bird2', 'camel', 'cat', 'centaur', 'dino_ske', 'dinosaur', 'dog1', 'dog2', 'flamingo', 'glasses', 'gorilla', 'hand', 'horse', 'lamp', 'laptop', 'man', 'myScissor', 'octopus', 'pliers', 'rabbit', 'santa', 'shark', 'snake', 'spiders', 'two_balls', 'woman']\n",
      "\n",
      "Classes when converted to idx:\n",
      "{'alien': 0, 'ants': 1, 'armadillo': 2, 'bird1': 3, 'bird2': 4, 'camel': 5, 'cat': 6, 'centaur': 7, 'dino_ske': 8, 'dinosaur': 9, 'dog1': 10, 'dog2': 11, 'flamingo': 12, 'glasses': 13, 'gorilla': 14, 'hand': 15, 'horse': 16, 'lamp': 17, 'laptop': 18, 'man': 19, 'myScissor': 20, 'octopus': 21, 'pliers': 22, 'rabbit': 23, 'santa': 24, 'shark': 25, 'snake': 26, 'spiders': 27, 'two_balls': 28, 'woman': 29}\n",
      "\n",
      "Opt Input Edges:  750\n"
     ]
    }
   ],
   "source": [
    "dataset.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')\n",
    "dataset.root = opt.dataroot\n",
    "dataset.dir = os.path.join(opt.dataroot)\n",
    "dataset.classes, dataset.class_to_idx = dataset.find_classes(dataset.dir)\n",
    "dataset.paths = dataset.make_dataset_by_class(dataset.dir, dataset.class_to_idx, opt.phase)\n",
    "dataset.nclasses = len(dataset.classes)\n",
    "dataset.size = len(dataset.paths)\n",
    "dataset.get_mean_std()\n",
    "print('The Device being using is', dataset.device)\n",
    "print('The Root Path is', dataset.root)\n",
    "print('Directory is', dataset.dir)\n",
    "print()\n",
    "print('Classes are:', dataset.classes)\n",
    "print()\n",
    "print('Classes when converted to idx:')\n",
    "print(dataset.class_to_idx)\n",
    "print()\n",
    "print('Opt Input Edges: ', dataset.opt.ninput_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0 \n",
    "path = dataset.paths[index][0]\n",
    "label = dataset.paths[index][1]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Path is datasets/shrec_16/alien/train/cache/T133_018.npz\n",
      "Path Exists: Loading the mesh data\n"
     ]
    }
   ],
   "source": [
    "mesh = Mesh(file=path, opt=dataset.opt, hold_history=False, export_folder= dataset.opt.export_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 750)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
