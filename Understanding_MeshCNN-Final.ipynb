{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i)  Peforming necessary imports, before writing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import time\n",
    "from data import DataLoader\n",
    "from models import create_model\n",
    "from util.writer import Writer\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "from util import util\n",
    "import torch\n",
    "import sys\n",
    "CUDA_LAUNCH_BLOCKING=\"1\"\n",
    "from os.path import join\n",
    "from models import networks\n",
    "from torch import nn\n",
    "from util.util import seg_accuracy, print_network\n",
    "from models.layers.mesh_conv import MeshConv\n",
    "import functools\n",
    "from models.layers.mesh_pool import MeshPool\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "from models.layers.mesh_union import MeshUnion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MeshPooling Operations: \n",
    "\n",
    "\n",
    "(A) Running the MeshPooling operation to obtain the pooled version of the mesh\n",
    "\n",
    "(B) The pooled mesh is implemented using a heap and a queue data structure deciding whichever has higher priority is implemented first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshPool(nn.Module):\n",
    "    # Initialize the important parameters\n",
    "    def __init__(self, target, multi_thread=False):\n",
    "        super(MeshPool, self).__init__()\n",
    "        self.__out_target = target\n",
    "        self.__multi_thread = multi_thread\n",
    "        self.__fe = None\n",
    "        self.__updated_fe = None\n",
    "        self.__meshes = None\n",
    "        self.__merge_edges = [-1, -1]\n",
    "\n",
    "    # Call function to obtain the forward pass function.\n",
    "    def __call__(self, fe, meshes):\n",
    "        return self.forward(fe, meshes)\n",
    "\n",
    "    # Forward pass function => for\n",
    "    def forward(self, fe, meshes):\n",
    "        self.__updated_fe = [[] for _ in range(len(meshes))]\n",
    "        pool_threads = []\n",
    "        self.__fe = fe\n",
    "        self.__meshes = meshes\n",
    "        # iterate over batch\n",
    "        for mesh_index in range(len(meshes)):\n",
    "            if self.__multi_thread:\n",
    "                pool_threads.append(Thread(target=self.__pool_main, args=(mesh_index,)))\n",
    "                pool_threads[-1].start()\n",
    "            else:\n",
    "                print('Mesh Index is', mesh_index)\n",
    "                self.__pool_main(mesh_index)\n",
    "        if self.__multi_thread:\n",
    "            for mesh_index in range(len(meshes)):\n",
    "                pool_threads[mesh_index].join()\n",
    "        out_features = torch.cat(self.__updated_fe).view(len(meshes), -1, self.__out_target)\n",
    "        return out_features\n",
    "\n",
    "    def __pool_main(self, mesh_index):\n",
    "        mesh = self.__meshes[mesh_index]\n",
    "        queue = self.__build_queue(self.__fe[mesh_index, :, :mesh.edges_count], mesh.edges_count)\n",
    "        # recycle = []\n",
    "        # last_queue_len = len(queue)\n",
    "        last_count = mesh.edges_count + 1\n",
    "        mask = np.ones(mesh.edges_count, dtype=np.bool)\n",
    "        edge_groups = MeshUnion(mesh.edges_count, self.__fe.device)\n",
    "        while mesh.edges_count > self.__out_target:\n",
    "            value, edge_id = heappop(queue)\n",
    "            edge_id = int(edge_id)\n",
    "            if mask[edge_id]:\n",
    "                self.__pool_edge(mesh, edge_id, mask, edge_groups)\n",
    "        mesh.clean(mask, edge_groups)\n",
    "        fe = edge_groups.rebuild_features(self.__fe[mesh_index], mask, self.__out_target)\n",
    "        self.__updated_fe[mesh_index] = fe\n",
    "\n",
    "    def __pool_edge(self, mesh, edge_id, mask, edge_groups):\n",
    "        if self.has_boundaries(mesh, edge_id):\n",
    "            return False\n",
    "        elif self.__clean_side(mesh, edge_id, mask, edge_groups, 0)\\\n",
    "            and self.__clean_side(mesh, edge_id, mask, edge_groups, 2) \\\n",
    "            and self.__is_one_ring_valid(mesh, edge_id):\n",
    "            self.__merge_edges[0] = self.__pool_side(mesh, edge_id, mask, edge_groups, 0)\n",
    "            self.__merge_edges[1] = self.__pool_side(mesh, edge_id, mask, edge_groups, 2)\n",
    "            mesh.merge_vertices(edge_id)\n",
    "            mask[edge_id] = False\n",
    "            MeshPool.__remove_group(mesh, edge_groups, edge_id)\n",
    "            mesh.edges_count -= 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __clean_side(self, mesh, edge_id, mask, edge_groups, side):\n",
    "        if mesh.edges_count <= self.__out_target:\n",
    "            return False\n",
    "        invalid_edges = MeshPool.__get_invalids(mesh, edge_id, edge_groups, side)\n",
    "        while len(invalid_edges) != 0 and mesh.edges_count > self.__out_target:\n",
    "            self.__remove_triplete(mesh, mask, edge_groups, invalid_edges)\n",
    "            if mesh.edges_count <= self.__out_target:\n",
    "                return False\n",
    "            if self.has_boundaries(mesh, edge_id):\n",
    "                return False\n",
    "            invalid_edges = self.__get_invalids(mesh, edge_id, edge_groups, side)\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def has_boundaries(mesh, edge_id):\n",
    "        for edge in mesh.gemm_edges[edge_id]:\n",
    "            if edge == -1 or -1 in mesh.gemm_edges[edge]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __is_one_ring_valid(mesh, edge_id):\n",
    "        v_a = set(mesh.edges[mesh.ve[mesh.edges[edge_id, 0]]].reshape(-1))\n",
    "        v_b = set(mesh.edges[mesh.ve[mesh.edges[edge_id, 1]]].reshape(-1))\n",
    "        shared = v_a & v_b - set(mesh.edges[edge_id])\n",
    "        return len(shared) == 2\n",
    "\n",
    "    def __pool_side(self, mesh, edge_id, mask, edge_groups, side):\n",
    "        info = MeshPool.__get_face_info(mesh, edge_id, side)\n",
    "        key_a, key_b, side_a, side_b, _, other_side_b, _, other_keys_b = info\n",
    "        self.__redirect_edges(mesh, key_a, side_a - side_a % 2, other_keys_b[0], mesh.sides[key_b, other_side_b])\n",
    "        self.__redirect_edges(mesh, key_a, side_a - side_a % 2 + 1, other_keys_b[1], mesh.sides[key_b, other_side_b + 1])\n",
    "        MeshPool.__union_groups(mesh, edge_groups, key_b, key_a)\n",
    "        MeshPool.__union_groups(mesh, edge_groups, edge_id, key_a)\n",
    "        mask[key_b] = False\n",
    "        MeshPool.__remove_group(mesh, edge_groups, key_b)\n",
    "        mesh.remove_edge(key_b)\n",
    "        mesh.edges_count -= 1\n",
    "        return key_a\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_invalids(mesh, edge_id, edge_groups, side):\n",
    "        info = MeshPool.__get_face_info(mesh, edge_id, side)\n",
    "        key_a, key_b, side_a, side_b, other_side_a, other_side_b, other_keys_a, other_keys_b = info\n",
    "        shared_items = MeshPool.__get_shared_items(other_keys_a, other_keys_b)\n",
    "        if len(shared_items) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            assert (len(shared_items) == 2)\n",
    "            middle_edge = other_keys_a[shared_items[0]]\n",
    "            update_key_a = other_keys_a[1 - shared_items[0]]\n",
    "            update_key_b = other_keys_b[1 - shared_items[1]]\n",
    "            update_side_a = mesh.sides[key_a, other_side_a + 1 - shared_items[0]]\n",
    "            update_side_b = mesh.sides[key_b, other_side_b + 1 - shared_items[1]]\n",
    "            MeshPool.__redirect_edges(mesh, edge_id, side, update_key_a, update_side_a)\n",
    "            MeshPool.__redirect_edges(mesh, edge_id, side + 1, update_key_b, update_side_b)\n",
    "            MeshPool.__redirect_edges(mesh, update_key_a, MeshPool.__get_other_side(update_side_a), update_key_b, MeshPool.__get_other_side(update_side_b))\n",
    "            MeshPool.__union_groups(mesh, edge_groups, key_a, edge_id)\n",
    "            MeshPool.__union_groups(mesh, edge_groups, key_b, edge_id)\n",
    "            MeshPool.__union_groups(mesh, edge_groups, key_a, update_key_a)\n",
    "            MeshPool.__union_groups(mesh, edge_groups, middle_edge, update_key_a)\n",
    "            MeshPool.__union_groups(mesh, edge_groups, key_b, update_key_b)\n",
    "            MeshPool.__union_groups(mesh, edge_groups, middle_edge, update_key_b)\n",
    "            return [key_a, key_b, middle_edge]\n",
    "\n",
    "    @staticmethod\n",
    "    def __redirect_edges(mesh, edge_a_key, side_a, edge_b_key, side_b):\n",
    "        mesh.gemm_edges[edge_a_key, side_a] = edge_b_key\n",
    "        mesh.gemm_edges[edge_b_key, side_b] = edge_a_key\n",
    "        mesh.sides[edge_a_key, side_a] = side_b\n",
    "        mesh.sides[edge_b_key, side_b] = side_a\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_shared_items(list_a, list_b):\n",
    "        shared_items = []\n",
    "        for i in range(len(list_a)):\n",
    "            for j in range(len(list_b)):\n",
    "                if list_a[i] == list_b[j]:\n",
    "                    shared_items.extend([i, j])\n",
    "        return shared_items\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_other_side(side):\n",
    "        return side + 1 - 2 * (side % 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_face_info(mesh, edge_id, side):\n",
    "        key_a = mesh.gemm_edges[edge_id, side]\n",
    "        key_b = mesh.gemm_edges[edge_id, side + 1]\n",
    "        side_a = mesh.sides[edge_id, side]\n",
    "        side_b = mesh.sides[edge_id, side + 1]\n",
    "        other_side_a = (side_a - (side_a % 2) + 2) % 4\n",
    "        other_side_b = (side_b - (side_b % 2) + 2) % 4\n",
    "        other_keys_a = [mesh.gemm_edges[key_a, other_side_a], mesh.gemm_edges[key_a, other_side_a + 1]]\n",
    "        other_keys_b = [mesh.gemm_edges[key_b, other_side_b], mesh.gemm_edges[key_b, other_side_b + 1]]\n",
    "        return key_a, key_b, side_a, side_b, other_side_a, other_side_b, other_keys_a, other_keys_b\n",
    "\n",
    "    @staticmethod\n",
    "    def __remove_triplete(mesh, mask, edge_groups, invalid_edges):\n",
    "        vertex = set(mesh.edges[invalid_edges[0]])\n",
    "        for edge_key in invalid_edges:\n",
    "            vertex &= set(mesh.edges[edge_key])\n",
    "            mask[edge_key] = False\n",
    "            MeshPool.__remove_group(mesh, edge_groups, edge_key)\n",
    "        mesh.edges_count -= 3\n",
    "        vertex = list(vertex)\n",
    "        assert(len(vertex) == 1)\n",
    "        mesh.remove_vertex(vertex[0])\n",
    "\n",
    "    def __build_queue(self, features, edges_count):\n",
    "        # delete edges with smallest norm\n",
    "        squared_magnitude = torch.sum(features * features, 0)\n",
    "        if squared_magnitude.shape[-1] != 1:\n",
    "            squared_magnitude = squared_magnitude.unsqueeze(-1)\n",
    "        edge_ids = torch.arange(edges_count, device=squared_magnitude.device, dtype=torch.float32).unsqueeze(-1)\n",
    "        heap = torch.cat((squared_magnitude, edge_ids), dim=-1).tolist()\n",
    "        heapify(heap)\n",
    "        return heap\n",
    "\n",
    "    @staticmethod\n",
    "    def __union_groups(mesh, edge_groups, source, target):\n",
    "        edge_groups.union(source, target)\n",
    "        mesh.union_groups(source, target)\n",
    "\n",
    "    @staticmethod\n",
    "    def __remove_group(mesh, edge_groups, index):\n",
    "        edge_groups.remove_group(index)\n",
    "        mesh.remove_group(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) This function is called right after Mesh ConvNet\n",
    "\n",
    "\n",
    "(i) Setattr and Getattr are used to obtain the forward convolution. \n",
    "\n",
    "(ii) The setattr first sets the value of the attribute, and getattr obtains that value. \n",
    "\n",
    "(iii) MeshPool changes the structure of the mesh completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshConvNet(nn.Module):\n",
    "    \"\"\"Global classification task using MeshConvNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, norm_layer, nf0, conv_res, nclasses, input_res, pool_res, fc_n,\n",
    "                 nresblocks=3):\n",
    "        print('Norm layer', norm_layer)\n",
    "        print('Nf0', nf0)\n",
    "        print('Number of Convolutional Filter', conv_res)\n",
    "        print('Number of Classes', nclasses)\n",
    "        print('Number of Input Edges',input_res)\n",
    "        print('Edge Resolution Across Layers', pool_res)\n",
    "        print('Number of nodes is penultimate fc layer', fc_n)\n",
    "        print('Number of Residual Blocks:: Skip Connections', nresblocks)\n",
    "        print('In Channels Start :', [nf0] + conv_res)\n",
    "        super(MeshConvNet, self).__init__()\n",
    "        self.k = [nf0] + conv_res\n",
    "        self.res = [input_res] + pool_res\n",
    "        norm_args = get_norm_args(norm_layer, self.k[1:])\n",
    "\n",
    "        for i, ki in enumerate(self.k[:-1]):\n",
    "            setattr(self, 'conv{}'.format(i), MResConv(ki, self.k[i + 1], nresblocks))\n",
    "            setattr(self, 'norm{}'.format(i), norm_layer(**norm_args[i]))\n",
    "            setattr(self, 'pool{}'.format(i), MeshPool(self.res[i + 1]))\n",
    "\n",
    "\n",
    "        self.gp = torch.nn.AvgPool1d(self.res[-1])\n",
    "        # self.gp = torch.nn.MaxPool1d(self.res[-1])\n",
    "        self.fc1 = nn.Linear(self.k[-1], fc_n)\n",
    "        self.fc2 = nn.Linear(fc_n, nclasses)\n",
    "\n",
    "    def forward(self, x, mesh):\n",
    "\n",
    "        # Mesh convolutional layers.\n",
    "        for i in range(len(self.k) - 1):\n",
    "            x = getattr(self, 'conv{}'.format(i))(x, mesh)\n",
    "            x = F.relu(getattr(self, 'norm{}'.format(i))(x))\n",
    "            x = getattr(self, 'pool{}'.format(i))(x, mesh)\n",
    "\n",
    "        # The final fully connected layers of the network. \n",
    "        x = self.gp(x)\n",
    "        x = x.view(-1, self.k[-1])\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Residual Convolutional Layers, the first possible layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Mesh-Residual convolution class. \n",
    "class MResConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skips=1):\n",
    "        super(MResConv, self).__init__()\n",
    "        self.in_channels = in_channels     \n",
    "        self.out_channels = out_channels\n",
    "        self.skips = skips\n",
    "        self.conv0 = MeshConv(self.in_channels, self.out_channels, bias=False)\n",
    "        for i in range(self.skips):\n",
    "            setattr(self, 'bn{}'.format(i + 1), nn.BatchNorm2d(self.out_channels))\n",
    "            setattr(self, 'conv{}'.format(i + 1),\n",
    "                    MeshConv(self.out_channels, self.out_channels, bias=False))\n",
    "\n",
    "    def forward(self, x, mesh):\n",
    "        x = self.conv0(x, mesh)\n",
    "        x1 = x\n",
    "        for i in range(self.skips):\n",
    "            x = getattr(self, 'bn{}'.format(i + 1))(F.relu(x))\n",
    "            x = getattr(self, 'conv{}'.format(i + 1))(x, mesh)\n",
    "        x += x1\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) The norm_layer for normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_layer(norm_type='instance', num_groups=1):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n",
    "    elif norm_type == 'group':\n",
    "        norm_layer = functools.partial(nn.GroupNorm, affine=True, num_groups=num_groups)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = NoNorm\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_args(norm_layer, nfeats_list):\n",
    "    if hasattr(norm_layer, '__name__') and norm_layer.__name__ == 'NoNorm':\n",
    "        norm_args = [{'fake': True} for f in nfeats_list]\n",
    "    elif norm_layer.func.__name__ == 'GroupNorm':\n",
    "        norm_args = [{'num_channels': f} for f in nfeats_list]\n",
    "    elif norm_layer.func.__name__ == 'BatchNorm':\n",
    "        norm_args = [{'num_features': f} for f in nfeats_list]\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_layer.func.__name__)\n",
    "    return norm_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) The initialization of the network, with random parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(net, init_type, init_gain, gpu_ids):\n",
    "    if len(gpu_ids) > 0:\n",
    "        assert(torch.cuda.is_available())\n",
    "        net.cuda(gpu_ids[0])\n",
    "        net = net.cuda()\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)\n",
    "    if init_type != 'none':\n",
    "        init_weights(net, init_type, init_gain)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) The Initialization algorithm of our network: Weight initalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type, init_gain):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (v)  Which Classifier to Use, Mesh-Conv-Net or Mesh-UNET for segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_classifier(input_nc, ncf, ninput_edges, nclasses, opt, gpu_ids, arch, init_type, init_gain):\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=opt.norm, num_groups=opt.num_groups)\n",
    "    if arch == 'mconvnet':\n",
    "        # Defining a convolutional mesh classifier. \n",
    "        net = MeshConvNet(norm_layer, input_nc, ncf, nclasses, ninput_edges, opt.pool_res, opt.fc_n,\n",
    "                          opt.resblocks)\n",
    "    elif arch == 'meshunet':\n",
    "        down_convs = [input_nc] + ncf\n",
    "        up_convs = ncf[::-1] + [nclasses]\n",
    "        pool_res = [ninput_edges] + opt.pool_res\n",
    "        net = MeshEncoderDecoder(pool_res, down_convs, up_convs, blocks=opt.resblocks,\n",
    "                                 transfer_data=True)\n",
    "    else:\n",
    "        raise NotImplementedError('Encoder model name [%s] is not recognized' % arch)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)  MeshConv: The fundamental building block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshConv(nn.Module):\n",
    "    \"\"\" Computes convolution between edges and 4 incident (1-ring) edge neighbors\n",
    "    in the forward pass takes:\n",
    "    x: edge features (Batch x Features x Edges)\n",
    "    mesh: list of mesh data-structure (len(mesh) == Batch)\n",
    "    and applies convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, k=5, bias=True):\n",
    "        super(MeshConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, k), bias=bias)\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, edge_f, mesh):\n",
    "        return self.forward(edge_f, mesh)\n",
    "\n",
    "    def forward(self, x, mesh):\n",
    "        print(x.shape)\n",
    "        x = x.squeeze(-1)\n",
    "        G = torch.cat([self.pad_gemm(i, x.shape[2], x.device) for i in mesh], 0)\n",
    "        # build 'neighborhood image' and apply convolution\n",
    "        G = self.create_GeMM(x, G)\n",
    "        # Build the neigbourhood and create_GEMM edges \n",
    "        x = self.conv(G)\n",
    "        return x\n",
    "\n",
    "    def flatten_gemm_inds(self, Gi):\n",
    "        (b, ne, nn) = Gi.shape\n",
    "        ne += 1\n",
    "        batch_n = torch.floor(torch.arange(b * ne, device=Gi.device).float() / ne).view(b, ne)\n",
    "        add_fac = batch_n * ne\n",
    "        add_fac = add_fac.view(b, ne, 1)\n",
    "        add_fac = add_fac.repeat(1, 1, nn)\n",
    "        # flatten Gi\n",
    "        Gi = Gi.float() + add_fac[:, 1:, :]\n",
    "        return Gi\n",
    "\n",
    "    # Create GEMM features from x and the graph structure G\n",
    "    def create_GeMM(self, x, Gi):\n",
    "        \"\"\" gathers the edge features (x) with from the 1-ring indices (Gi)\n",
    "        applys symmetric functions to handle order invariance\n",
    "        returns a 'fake image' which can use 2d convolution on\n",
    "        output dimensions: Batch x Channels x Edges x 5\n",
    "        \"\"\"\n",
    "        Gishape = Gi.shape\n",
    "        # pad the first row of  every sample in batch with zeros\n",
    "        padding = torch.zeros((x.shape[0], x.shape[1], 1), requires_grad=True, device=x.device)\n",
    "        # padding = padding.to(x.device)\n",
    "        x = torch.cat((padding, x), dim=2)\n",
    "        Gi = Gi + 1 #shift\n",
    "\n",
    "        # Flatten Gemm indicies. \n",
    "        Gi_flat = self.flatten_gemm_inds(Gi)\n",
    "        Gi_flat = Gi_flat.view(-1).long()\n",
    "        \n",
    "        # Odim is shape of X. \n",
    "        odim = x.shape\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x = x.view(odim[0] * odim[2], odim[1])\n",
    "\n",
    "        # Index = Gi_flat is being selected. \n",
    "        \n",
    "        # Selecting the Gemm indices including current vertices. \n",
    "        f = torch.index_select(x, dim=0, index=Gi_flat)\n",
    "        \n",
    "        # Bring it back to the original shape and permute it. \n",
    "        f = f.view(Gishape[0], Gishape[1], Gishape[2], -1)\n",
    "        f = f.permute(0, 3, 1, 2)\n",
    "\n",
    "        # apply the symmetric functions for an equivariant conv\n",
    "        x_1 = f[:, :, :, 1] + f[:, :, :, 3]\n",
    "        x_2 = f[:, :, :, 2] + f[:, :, :, 4]\n",
    "        x_3 = torch.abs(f[:, :, :, 1] - f[:, :, :, 3])\n",
    "        x_4 = torch.abs(f[:, :, :, 2] - f[:, :, :, 4])\n",
    "        f = torch.stack([f[:, :, :, 0], x_1, x_2, x_3, x_4], dim=3)\n",
    "        return f\n",
    "\n",
    "    def pad_gemm(self, m, xsz, device):\n",
    "        \"\"\" extracts one-ring neighbors (4x) -> m.gemm_edges\n",
    "        which is of size #edges x 4\n",
    "        add the edge_id itself to make #edges x 5\n",
    "        then pad to desired size e.g., xsz x 5\n",
    "        \"\"\"\n",
    "        padded_gemm = torch.tensor(m.gemm_edges, device=device).float()\n",
    "        padded_gemm = padded_gemm.requires_grad_()\n",
    "        padded_gemm = torch.cat((torch.arange(m.edges_count, device=device).float().unsqueeze(1), padded_gemm), dim=1)\n",
    "        # pad using F\n",
    "        padded_gemm = F.pad(padded_gemm, (0, 0, 0, xsz - m.edges_count), \"constant\", 0)\n",
    "        padded_gemm = padded_gemm.unsqueeze(0)\n",
    "        return padded_gemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  The Classifier Model Class\n",
    "\n",
    "This model contains \n",
    "> (i) Data to be processed <br />\n",
    "> (ii) The classifier definition <br />\n",
    "> (iii) Backprop algorithm <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel:\n",
    "    \"\"\" Class for training Model weights\n",
    "    :args opt: structure containing configuration params\n",
    "    e.g.,\n",
    "    --dataset_mode -> classification / segmentation)\n",
    "    --arch -> network type                                                                       \n",
    "    \"\"\"\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        self.is_train = opt.is_train\n",
    "        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n",
    "        self.save_dir = join(opt.checkpoints_dir, opt.name)\n",
    "        self.optimizer = None\n",
    "        self.edge_features = None\n",
    "        self.labels = None\n",
    "        self.mesh = None\n",
    "        self.soft_label = None\n",
    "        self.loss = None\n",
    "\n",
    "        #\n",
    "        self.nclasses = opt.nclasses\n",
    "\n",
    "        # load/define networks: We import networks from the networks function.\n",
    "\n",
    "        # Define the network here. \n",
    "        self.net = define_classifier(opt.input_nc, opt.ncf, opt.ninput_edges, opt.nclasses, opt,\n",
    "                                              self.gpu_ids, opt.arch, opt.init_type, opt.init_gain)\n",
    "\n",
    "        # Check if the network is in training mode or not. \n",
    "        self.net.train(self.is_train)\n",
    "\n",
    "        self.criterion = networks.define_loss(opt).to(self.device)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.optimizer = torch.optim.Adam(self.net.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.scheduler = networks.get_scheduler(self.optimizer, opt)\n",
    "            print_network(self.net)\n",
    "\n",
    "        if not self.is_train or opt.continue_train:\n",
    "            self.load_network(opt.which_epoch)\n",
    "\n",
    "    #Set Input as self.set_input  \n",
    "    def set_input(self, data):\n",
    "        input_edge_features = torch.from_numpy(data['edge_features']).float()\n",
    "        labels = torch.from_numpy(data['label']).long()\n",
    "        self.edge_features = input_edge_features.to(self.device).requires_grad_(self.is_train)\n",
    "        self.labels = labels.to(self.device)\n",
    "        self.mesh = data['mesh']\n",
    "        if self.opt.dataset_mode == 'segmentation' and not self.is_train:\n",
    "            self.soft_label = torch.from_numpy(data['soft_label'])\n",
    "\n",
    "    def forward(self):\n",
    "        out = self.net(self.edge_features, self.mesh)\n",
    "        return out\n",
    "\n",
    "    def backward(self, out):\n",
    "        self.loss = self.criterion(out, self.labels)\n",
    "        self.loss.backward()\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.forward()\n",
    "        self.backward(out)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "##################\n",
    "\n",
    "    # Load network, from pretrained .pth file. \n",
    "    def load_network(self, which_epoch):\n",
    "        \"\"\"load model from disk\"\"\"\n",
    "        save_filename = '%s_net.pth' % which_epoch\n",
    "        load_path = join(self.save_dir, save_filename)\n",
    "        net = self.net\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net = net.module\n",
    "        print('loading the model from %s' % load_path)\n",
    "        # PyTorch newer than 0.4 (e.g., built from\n",
    "        # GitHub source), you can remove str() on self.device\n",
    "        state_dict = torch.load(load_path, map_location=str(self.device))\n",
    "        if hasattr(state_dict, '_metadata'):\n",
    "            del state_dict._metadata\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "    # Save the model after training after every epoch.\n",
    "    def save_network(self, which_epoch):\n",
    "        \"\"\"save model to disk\"\"\"\n",
    "        save_filename = '%s_net.pth' % (which_epoch)\n",
    "        save_path = join(self.save_dir, save_filename)\n",
    "        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "            torch.save(self.net.module.cpu().state_dict(), save_path)\n",
    "            self.net.cuda(self.gpu_ids[0])\n",
    "        else:\n",
    "            torch.save(self.net.cpu().state_dict(), save_path)\n",
    "\n",
    "    #Update the learning rate called every once in a while.\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"update learning rate (called once every epoch)\"\"\"\n",
    "        self.scheduler.step()\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        print('learning rate = %.7f' % lr)\n",
    "\n",
    "    # Test the model obtained\n",
    "    def test(self):\n",
    "        \"\"\"tests model\n",
    "        returns: number correct and total number\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            out = self.forward()\n",
    "            # compute number of correct\n",
    "            pred_class = out.data.max(1)[1]\n",
    "            label_class = self.labels\n",
    "            self.export_segmentation(pred_class.cpu())\n",
    "            correct = self.get_accuracy(pred_class, label_class)\n",
    "        return correct, len(label_class)\n",
    "\n",
    "    # Obtain accuracy of prediction.\n",
    "    def get_accuracy(self, pred, labels):\n",
    "        \"\"\"computes accuracy for classification / segmentation \"\"\"\n",
    "        if self.opt.dataset_mode == 'classification':\n",
    "            correct = pred.eq(labels).sum()\n",
    "        elif self.opt.dataset_mode == 'segmentation':\n",
    "            correct = seg_accuracy(pred, self.soft_label, self.mesh)\n",
    "        return correct\n",
    "\n",
    "    # Export and save segmentation. \n",
    "    def export_segmentation(self, pred_seg):\n",
    "        if self.opt.dataset_mode == 'segmentation':\n",
    "            for meshi, mesh in enumerate(self.mesh):\n",
    "                mesh.export_segments(pred_seg[meshi, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iii) Using Argument Parser with Default Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOptions:\n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self):\n",
    "        # data params\n",
    "        self.parser.add_argument('--dataroot', default = 'datasets/shrec_16', help='path to meshes (should have subfolders train, test)')\n",
    "        self.parser.add_argument('--dataset_mode', choices={\"classification\", \"segmentation\"}, default='classification')\n",
    "        self.parser.add_argument('--ninput_edges', type=int, default=750, help='# of input edges (will include dummy edges)')\n",
    "        self.parser.add_argument('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples per epoch')\n",
    "        # network params\n",
    "        self.parser.add_argument('--batch_size', type=int, default=16, help='input batch size')\n",
    "        self.parser.add_argument('--arch', type=str, default='mconvnet', help='selects network to use') #todo add choices\n",
    "        self.parser.add_argument('--resblocks', type=int, default=1, help='# of res blocks')\n",
    "        self.parser.add_argument('--fc_n', type=int, default=100, help='# between fc and nclasses') #todo make generic\n",
    "        self.parser.add_argument('--ncf', nargs='+', default=[64, 128, 256, 256], type=int, help='conv filters')\n",
    "        self.parser.add_argument('--pool_res', nargs='+', default= [600, 450, 300, 180], type=int, help='pooling res')\n",
    "        self.parser.add_argument('--norm', type=str, default='group',help='instance normalization or batch normalization or group normalization')\n",
    "        self.parser.add_argument('--num_groups', type=int, default=16, help='# of groups for groupnorm')\n",
    "        self.parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
    "        self.parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
    "        # general params\n",
    "        self.parser.add_argument('--num_threads', default=3, type=int, help='# threads for loading data')\n",
    "        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "        self.parser.add_argument('--name', type=str, default='debug', help='name of the experiment. It decides where to store samples and models')\n",
    "        self.parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
    "        self.parser.add_argument('--serial_batches', action='store_true', help='if true, takes meshes in order, otherwise takes them randomly')\n",
    "        self.parser.add_argument('--seed', type=int, help='if specified, uses seed')\n",
    "        # visualization params\n",
    "        self.parser.add_argument('--export_folder', type=str, default='', help='exports intermediate collapses to this folder')\n",
    "        #\n",
    "        self.initialized = True\n",
    "\n",
    "    def parse(self):\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        self.opt, unknown = self.parser.parse_known_args()\n",
    "        self.opt.is_train = self.is_train   # train or test\n",
    "\n",
    "        str_ids = self.opt.gpu_ids.split(',')\n",
    "        self.opt.gpu_ids = []\n",
    "        for str_id in str_ids:\n",
    "            id = int(str_id)\n",
    "            if id >= 0:\n",
    "                self.opt.gpu_ids.append(id)\n",
    "        # set gpu ids\n",
    "        if len(self.opt.gpu_ids) > 0:\n",
    "            torch.cuda.set_device(self.opt.gpu_ids[0])\n",
    "\n",
    "        args = vars(self.opt)\n",
    "\n",
    "        if self.opt.seed is not None:\n",
    "            import numpy as np\n",
    "            import random\n",
    "            torch.manual_seed(self.opt.seed)\n",
    "            np.random.seed(self.opt.seed)\n",
    "            random.seed(self.opt.seed)\n",
    "\n",
    "        if self.opt.export_folder:\n",
    "            self.opt.export_folder = os.path.join(self.opt.checkpoints_dir, self.opt.name, self.opt.export_folder)\n",
    "            util.mkdir(self.opt.export_folder)\n",
    "\n",
    "        if self.is_train:\n",
    "            print('------------ Options -------------')\n",
    "            for k, v in sorted(args.items()):\n",
    "                print('%s: %s' % (str(k), str(v)))\n",
    "            print('-------------- End ----------------')\n",
    "\n",
    "            # save to the disk\n",
    "            expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n",
    "            util.mkdir(expr_dir)\n",
    "\n",
    "            file_name = os.path.join(expr_dir, 'opt.txt')\n",
    "            with open(file_name, 'wt') as opt_file:\n",
    "                opt_file.write('------------ Options -------------\\n')\n",
    "                for k, v in sorted(args.items()):\n",
    "                    opt_file.write('%s: %s\\n' % (str(k), str(v)))\n",
    "                opt_file.write('-------------- End ----------------\\n')\n",
    "        return self.opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainOptions(BaseOptions):\n",
    "    def initialize(self):\n",
    "        BaseOptions.initialize(self)\n",
    "        self.parser.add_argument('--print_freq', type=int, default=10, help='frequency of showing training results on console')\n",
    "        self.parser.add_argument('--save_latest_freq', type=int, default=250, help='frequency of saving the latest results')\n",
    "        self.parser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\n",
    "        self.parser.add_argument('--run_test_freq', type=int, default=1, help='frequency of running test in training script')\n",
    "        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
    "        self.parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
    "        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
    "        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
    "        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n",
    "        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n",
    "        self.parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')\n",
    "        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
    "        self.parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n",
    "        self.parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n",
    "        # data augmentation stuff\n",
    "        self.parser.add_argument('--num_aug', type=int, default=20, help='# of augmentation files')\n",
    "        self.parser.add_argument('--scale_verts', action='store_true', help='non-uniformly scale the mesh e.g., in x, y or z')\n",
    "        self.parser.add_argument('--slide_verts', type=float, default=0.2, help='percent vertices which will be shifted along the mesh surface')\n",
    "        self.parser.add_argument('--flip_edges', type=float, default=0.2, help='percent of edges to randomly flip')\n",
    "        # tensorboard visualization\n",
    "        self.parser.add_argument('--no_vis', action='store_true', help='will not use tensorboard')\n",
    "        self.parser.add_argument('--verbose_plot', action='store_true', help='plots network weights, etc.')\n",
    "        self.is_train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Calling Parameters from Argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "arch: mconvnet\n",
      "batch_size: 16\n",
      "beta1: 0.9\n",
      "checkpoints_dir: ./checkpoints\n",
      "continue_train: False\n",
      "dataroot: datasets/shrec_16\n",
      "dataset_mode: classification\n",
      "epoch_count: 1\n",
      "export_folder: \n",
      "fc_n: 100\n",
      "flip_edges: 0.2\n",
      "gpu_ids: [0]\n",
      "init_gain: 0.02\n",
      "init_type: normal\n",
      "is_train: True\n",
      "lr: 0.0002\n",
      "lr_decay_iters: 50\n",
      "lr_policy: lambda\n",
      "max_dataset_size: inf\n",
      "name: debug\n",
      "ncf: [64, 128, 256, 256]\n",
      "ninput_edges: 750\n",
      "niter: 100\n",
      "niter_decay: 100\n",
      "no_vis: False\n",
      "norm: group\n",
      "num_aug: 20\n",
      "num_groups: 16\n",
      "num_threads: 3\n",
      "phase: train\n",
      "pool_res: [600, 450, 300, 180]\n",
      "print_freq: 10\n",
      "resblocks: 1\n",
      "run_test_freq: 1\n",
      "save_epoch_freq: 1\n",
      "save_latest_freq: 250\n",
      "scale_verts: False\n",
      "seed: None\n",
      "serial_batches: False\n",
      "slide_verts: 0.2\n",
      "verbose_plot: False\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "opt = TrainOptions().parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Initializing the dataset and network\n",
    " (i) model.net => the entire network architecture <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "Dataset Size is:  480\n",
      "Norm layer functools.partial(<class 'torch.nn.modules.normalization.GroupNorm'>, affine=True, num_groups=16)\n",
      "Nf0 5\n",
      "Number of Convolutional Filter [64, 128, 256, 256]\n",
      "Number of Classes 30\n",
      "Number of Input Edges 750\n",
      "Edge Resolution Across Layers [600, 450, 300, 180]\n",
      "Number of nodes is penultimate fc layer 100\n",
      "Number of Residual Blocks:: Skip Connections 1\n",
      "In Channels Start : [5, 64, 128, 256, 256]\n",
      "loading the model from ./checkpoints/debug/latest_net.pth\n"
     ]
    }
   ],
   "source": [
    "opt.is_train = False\n",
    "dataset = DataLoader(opt)\n",
    "print('Dataset Size is: ', len(dataset))\n",
    "model = ClassifierModel(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Paradigm \n",
    "\n",
    " - Writer, is a Summary Writer to note the important values during backpropogation.  <br />  <br />\n",
    "\n",
    " - model.set_input(data) => Sets the input data of the network <br />  <br />\n",
    "\n",
    " - model.optimize_parameters => A call to optimize the network parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/miniconda3/envs/meshcnn/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/home/abhinav/miniconda3/envs/meshcnn/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/home/abhinav/miniconda3/envs/meshcnn/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d18c3cc0bf83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepoch_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-821af27b3bb4>\u001b[0m in \u001b[0;36moptimize_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_grad'"
     ]
    }
   ],
   "source": [
    "writer = Writer(opt)\n",
    "total_steps = 0\n",
    "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    iter_data_time = time.time()\n",
    "    epoch_iter = 0\n",
    "    for i, data in enumerate(dataset):\n",
    "        iter_start_time = time.time()\n",
    "        if total_steps % opt.print_freq == 0:\n",
    "            t_data = iter_start_time - iter_data_time\n",
    "        total_steps += opt.batch_size\n",
    "        epoch_iter += opt.batch_size\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        if total_steps % opt.print_freq == 0:\n",
    "            loss = model.loss\n",
    "            t = (time.time() - iter_start_time) / opt.batch_size\n",
    "            writer.print_current_losses(epoch, epoch_iter, loss, t, t_data)\n",
    "            writer.plot_loss(loss, epoch, epoch_iter, dataset_size)\n",
    "\n",
    "        if i % opt.save_latest_freq == 0:\n",
    "            print('saving the latest model (epoch %d, total_steps %d)' %\n",
    "                      (epoch, total_steps))\n",
    "            model.save_network('latest')\n",
    "\n",
    "        iter_data_time = time.time()\n",
    "    if epoch % opt.save_epoch_freq == 0:\n",
    "        print('saving the model at the end of epoch %d, iters %d' %\n",
    "                  (epoch, total_steps))\n",
    "        model.save_network('latest')\n",
    "        model.save_network(epoch)\n",
    "\n",
    "    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "    model.update_learning_rate()\n",
    "    if opt.verbose_plot:\n",
    "        writer.plot_model_wts(model, epoch)\n",
    "\n",
    "    \n",
    "    if epoch % opt.run_test_freq == 0:\n",
    "        acc = run_test(epoch)\n",
    "        writer.plot_acc(acc, epoch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MeshConvNet(\n",
       "    (conv0): MResConv(\n",
       "      (conv0): MeshConv(\n",
       "        (conv): Conv2d(5, 64, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): MeshConv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm0): GroupNorm(16, 64, eps=1e-05, affine=True)\n",
       "    (pool0): MeshPool()\n",
       "    (conv1): MResConv(\n",
       "      (conv0): MeshConv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): MeshConv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm1): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
       "    (pool1): MeshPool()\n",
       "    (conv2): MResConv(\n",
       "      (conv0): MeshConv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): MeshConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "    (pool2): MeshPool()\n",
       "    (conv3): MResConv(\n",
       "      (conv0): MeshConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): MeshConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "    (pool3): MeshPool()\n",
       "    (gp): AvgPool1d(kernel_size=(180,), stride=(180,), padding=(0,))\n",
       "    (fc1): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc2): Linear(in_features=100, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1: A MeshConv Layer: \n",
    "\n",
    "#### MResConv + Group Norm + MeshPool\n",
    "\n",
    "- Input Edge features = (bs, 5, 750)\n",
    "\n",
    "- Model mesh \n",
    "\n",
    "- Input Size  = (edges) => (16,5,750) + (mesh) \n",
    "\n",
    "- The Mesh is nothing but a (mesh.obj) file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1: Output mesh has 64 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 750])\n",
      "torch.Size([16, 64, 750, 1])\n"
     ]
    }
   ],
   "source": [
    "out1 = model.net.module.conv0(model.edge_features,model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model.net.module.norm0(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model.net.module.pool0(out1, model.mesh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 600])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2: A Mesh Conv in residual format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model.net.module.conv1(out1,model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model.net.module.norm1(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model.net.module.pool1(out2, model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer3: A Similar MeshConv residual layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = model.net.module.conv2(out2,model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = model.net.module.norm2(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = model.net.module.pool2(out3, model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 4: The last MeshConv Layer before final MLP classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out4 = model.net.module.conv3(out3,model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out4 = model.net.module.norm3(out4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out4 = model.net.module.pool3(out4, model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 5: Average Pooling and Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out5 = model.net.module.gp(out4)\n",
    "out5 = out5.view(-1, model.net.module.k[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 6: Obtaining the final classified layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out6 = F.relu(model.net.module.fc1(out5))\n",
    "out6 = model.net.module.fc2(out6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeshResConv: => Next Layer of Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter1 = model.net.module.conv0.conv0(model.edge_features, model.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(model.net.module.conv0.skips):\n",
    "        x = getattr(model.net.module.conv0, 'bn{}'.format(i + 1))(F.relu(x))\n",
    "        x = getattr(model.net.module.conv0, 'conv{}'.format(i + 1))(x, model.mesh)\n",
    "x += inter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meshconv.png'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(inter1).render(\"meshconv\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smallest Layer: The Mesh Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input(data)\n",
    "x = model.edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mesh = model.mesh[0] #A sample mesh for forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsz = x.shape[2] #Number of edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mesh.gemm_edges.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Entering the padd_gemm function inside the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_gemm = torch.tensor(sample_mesh.gemm_edges, device= x.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Adding a numbered list as the edges list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_gemm = padded_gemm.requires_grad_()\n",
    "padded_gemm = torch.cat((torch.arange(sample_mesh.edges_count, device=x.device).float().unsqueeze(1), padded_gemm), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Padding to ensure size is same as edge count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_gemm = F.pad(padded_gemm, (0, 0, 0, xsz - sample_mesh.edges_count), \"constant\", 0) #Adding zeros to complete the padding\n",
    "padded_gemm = padded_gemm.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 750, 5])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_gemm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Assigning padded_gemm to a variable Gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gi = padded_gemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Proceeding to the create_GeMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gishape = Gi.shape\n",
    "\n",
    "# pad the first row of  every sample in batch with zeros\n",
    "padding = torch.zeros((x.shape[0], x.shape[1], 1), requires_grad=True, device=x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Flattening the GeMM indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding = padding.to(x.device)\n",
    "y  = x\n",
    "y = torch.cat((padding, y), dim=2)\n",
    "Gi = Gi + 1 #shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gi_flat = model.net.module.conv0.conv0.flatten_gemm_inds(Gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "(b, ne, nn) = Gi.shape\n",
    "ne += 1\n",
    "batch_n = torch.floor(torch.arange(b * ne, device=Gi.device).float() / ne).view(b, ne)\n",
    "add_fac = batch_n * ne\n",
    "add_fac = add_fac.view(b, ne, 1)\n",
    "add_fac = add_fac.repeat(1, 1, nn)\n",
    "# flatten Gi\n",
    "Gi_flat = Gi.float() + add_fac[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 751, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_fac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first flatten indices\n",
    "Gi_flat = Gi_flat.view(-1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 750, 5])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Applying permuations: To switch the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "odim = y.shape\n",
    "y = y.permute(0, 2, 1).contiguous()\n",
    "y = y.view(odim[0] * odim[2], odim[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.index_select(y, dim=0, index=Gi_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 750, 5])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f.view(Gishape[0], Gishape[1], Gishape[2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the symmetric functions for an equivariant conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = f[:, :, :, 1] + f[:, :, :, 3]\n",
    "x_2 = f[:, :, :, 2] + f[:, :, :, 4]\n",
    "x_3 = torch.abs(f[:, :, :, 1] - f[:, :, :, 3])\n",
    "x_4 = torch.abs(f[:, :, :, 2] - f[:, :, :, 4])\n",
    "f = torch.stack([f[:, :, :, 0], x_1, x_2, x_3, x_4], dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is followed by a simple convolution operation: to obtain the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 750, 5])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1., 104., 141., 154.,  55.],\n",
       "         [  2.,   3.,  84., 112., 148.],\n",
       "         [  3.,  84.,   2., 107., 158.],\n",
       "         ...,\n",
       "         [  1.,   1.,   1.,   1.,   1.],\n",
       "         [  1.,   1.,   1.,   1.,   1.],\n",
       "         [  1.,   1.,   1.,   1.,   1.]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
